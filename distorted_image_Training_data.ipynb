{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsS7GDDT+6Qovq1Rdpzqr/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gxwthmmmm/Medical-Image-Distortion/blob/training/distorted_image_Training_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINING THE MODEL"
      ],
      "metadata": {
        "id": "3JtgyyfG1Pod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import shutil\n",
        "from PIL import Image\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Loading  base image\n",
        "base_image_path = \"brain.png\"\n",
        "if not os.path.exists(base_image_path):\n",
        "    print(f\"Image '{base_image_path}' not found.\")\n",
        "    img_np = None\n",
        "else:\n",
        "    img = Image.open(base_image_path).convert(\"L\")\n",
        "    img_np = np.array(img)\n",
        "\n",
        "if img_np is not None:\n",
        "    # Settings\n",
        "    noise_levels = [10, 20, 30, 40, 50]  # std_dev for Gaussian noise\n",
        "    num_per_level = 80  # 80 images per class\n",
        "    img_size = (128, 128)\n",
        "\n",
        "    # Clean previous dataset\n",
        "    dataset_dir = \"dataset_gaussian\"\n",
        "    if os.path.exists(dataset_dir):\n",
        "        shutil.rmtree(dataset_dir)\n",
        "\n",
        "    for i, std_dev in enumerate(noise_levels):\n",
        "        level_dir = os.path.join(dataset_dir, f\"Noise_Level_{i+1}\")\n",
        "        os.makedirs(level_dir, exist_ok=True)\n",
        "\n",
        "        for j in range(num_per_level):\n",
        "            noisy_img = img_np + np.random.normal(0, std_dev, img_np.shape)\n",
        "            noisy_img = np.clip(noisy_img, 0, 255).astype(np.uint8)\n",
        "\n",
        "            # Resize and save\n",
        "            resized = cv2.resize(noisy_img, img_size)\n",
        "            cv2.imwrite(os.path.join(level_dir, f\"noisy_{j+1:03}.png\"), resized)\n",
        "\n",
        "    print(\" Dataset generated with clean Gaussian noise levels.\")\n",
        "\n",
        "    # Loading data\n",
        "    batch_size = 16\n",
        "    datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        validation_split=0.2\n",
        "    )\n",
        "\n",
        "    train_data = datagen.flow_from_directory(\n",
        "        dataset_dir,\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        color_mode='grayscale',\n",
        "        class_mode='categorical',\n",
        "        subset='training'\n",
        "    )\n",
        "\n",
        "    val_data = datagen.flow_from_directory(\n",
        "        dataset_dir,\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        color_mode='grayscale',\n",
        "        class_mode='categorical',\n",
        "        subset='validation'\n",
        "    )\n",
        "\n",
        "    # Defining a  deeper CNN\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),\n",
        "        layers.MaxPooling2D(2, 2),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D(2, 2),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(train_data.num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    print(\"\\n Training model on clean Gaussian noise levels...\")\n",
        "    model.fit(train_data, epochs=10, validation_data=val_data)\n",
        "\n",
        "    # Evaluation the model\n",
        "    loss, acc = model.evaluate(val_data)\n",
        "    print(f\"\\n Final Validation Accuracy: {acc:.4f}\")\n",
        "else:\n",
        "    print(\"Base image could not be loaded. Exiting.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixmD3HqG1PxV",
        "outputId": "6ddac968-b8df-4698-b13e-35ac3e262b38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Dataset generated with clean Gaussian noise levels.\n",
            "Found 320 images belonging to 5 classes.\n",
            "Found 80 images belonging to 5 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Training model on clean Gaussian noise levels...\n",
            "Epoch 1/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 511ms/step - accuracy: 0.2086 - loss: 1.7654 - val_accuracy: 0.4000 - val_loss: 1.4237\n",
            "Epoch 2/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 531ms/step - accuracy: 0.3903 - loss: 1.3086 - val_accuracy: 0.8000 - val_loss: 0.8478\n",
            "Epoch 3/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 536ms/step - accuracy: 0.8457 - loss: 0.7279 - val_accuracy: 0.8000 - val_loss: 0.3139\n",
            "Epoch 4/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 589ms/step - accuracy: 0.9488 - loss: 0.1892 - val_accuracy: 1.0000 - val_loss: 0.0898\n",
            "Epoch 5/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 480ms/step - accuracy: 0.8425 - loss: 0.4028 - val_accuracy: 1.0000 - val_loss: 0.0492\n",
            "Epoch 6/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 524ms/step - accuracy: 1.0000 - loss: 0.0592 - val_accuracy: 1.0000 - val_loss: 0.0211\n",
            "Epoch 7/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 529ms/step - accuracy: 1.0000 - loss: 0.0137 - val_accuracy: 1.0000 - val_loss: 0.0067\n",
            "Epoch 8/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 567ms/step - accuracy: 1.0000 - loss: 0.0060 - val_accuracy: 1.0000 - val_loss: 0.0046\n",
            "Epoch 9/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 493ms/step - accuracy: 1.0000 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 0.0037\n",
            "Epoch 10/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 418ms/step - accuracy: 1.0000 - loss: 0.0035"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the accuracy\n"
      ],
      "metadata": {
        "id": "FKoGokrp1QDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on validation data\n",
        "val_loss, val_accuracy = model.evaluate(val_data)\n",
        "print(f\"\\nValidation Accuracy: {val_accuracy:.1f}\")\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17DH9if21QMl",
        "outputId": "ce63e8a5-7e83-4f83-a3f6-10c59c361458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - accuracy: 1.0000 - loss: 7.5662e-04\n",
            "\n",
            "Validation Accuracy: 1.0\n",
            "Validation Loss: 0.0008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the trained data to check the noise level of an Image"
      ],
      "metadata": {
        "id": "F8-LA9ag1QWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Load a new test image (grayscale)\n",
        "test_img = Image.open(\"/content/brain_mri_1.png.png\").convert(\"L\")\n",
        "test_img = test_img.resize((128, 128))\n",
        "test_array = np.array(test_img).astype(\"float32\") / 255.0\n",
        "test_array = np.expand_dims(test_array, axis=(0, -1))  # Shape: (1, 128, 128, 1)\n",
        "\n",
        "# Predict the noise level of the image\n",
        "prediction = model.predict(test_array)\n",
        "predicted_class = np.argmax(prediction)\n",
        "\n",
        "# Displays which noise level is the image\n",
        "plt.imshow(test_img, cmap='gray')\n",
        "plt.title(f\"Predicted Noise Level: {predicted_class + 1}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n8Fe21_X1Qcn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}